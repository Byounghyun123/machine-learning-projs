{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzLGKK8o9vn3xVuZPT9wcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byounghyun123/machine-learning-projs/blob/main/Rock_Scissors_Paper_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rock Scissors Paper Classification (Multi-Class Classification Model)**"
      ],
      "metadata": {
        "id": "M9xRX3x6_D5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improved upon the binary-class classification model, this model will be able to classify multiple classes. In this project, the aim is to distinguish images of rock, scissors, and paper. The dataset will cover multiple variations of hand (i.e. race), further preventing the likelihood of overfitting."
      ],
      "metadata": {
        "id": "1w7SiKq2_MyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the Dataset**"
      ],
      "metadata": {
        "id": "ZBhVVs7BA6Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using a public Rock-Paper-Scissors dataset, a gallery of hands images in Rock, Paper, and Scissors poses."
      ],
      "metadata": {
        "id": "Qopf48H9BBSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the train set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps.zip\n",
        "\n",
        "# Download the test set\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week4/rps-test-set.zip"
      ],
      "metadata": {
        "id": "lxXACLotBGBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Extract the archive\n",
        "local_zip = './rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/rps-train')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = './rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('tmp/rps-test')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "s8Okx5uxBJF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the directory names into variables and look at the filenames as a sanity check."
      ],
      "metadata": {
        "id": "zJQFDt-LBMTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = 'tmp/rps-train/rps'\n",
        "\n",
        "rock_dir = os.path.join(base_dir, 'rock')\n",
        "paper_dir = os.path.join(base_dir, 'paper')\n",
        "scissors_dir = os.path.join(base_dir, 'scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])"
      ],
      "metadata": {
        "id": "S41ZkidiBOL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can also inspect some of the images to see the variety of model inputs."
      ],
      "metadata": {
        "id": "_nMPZ9Q1BdfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_rock = [os.path.join(rock_dir, fname)\n",
        "                for fname in rock_files[pic_index-2:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname)\n",
        "                for fname in paper_files[pic_index-2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname)\n",
        "                for fname in scissors_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jy3eYTZCBhtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the Model**"
      ],
      "metadata": {
        "id": "5uBJFNkIBjBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use 4 convolution layers with 64-64-128-128 filters then append a Dropout layer to avoid overfitting and some Dense layers for the classification. The output layer would be a 3-neuron dense layer activated by Softmax. It scales the output to a set of probabilities that add up to 1. The order of this 3-neuron output would be paper-rock-scissors (e.g. a [0.8 0.2 0.0] output means the model is prediciting 80% probability for paper and 20% probability for rock. The model.summary() below enables us to examine the architecture of the model. The loss function used in this model would be categorical_crossentropy."
      ],
      "metadata": {
        "id": "Zolo25wEBk97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gMHwwfbiB-Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the training parameters\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1pOSj7ZVCIKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the ImageDataGenerator**"
      ],
      "metadata": {
        "id": "FTiBV5l5CQ9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"tmp/rps-train/rps\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"tmp/rps-test/rps-test-set\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")"
      ],
      "metadata": {
        "id": "bSK2i29qCVFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**"
      ],
      "metadata": {
        "id": "rtI2K4VNCXvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model for 25 epochs."
      ],
      "metadata": {
        "id": "LEG3gtwxCaw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=25, steps_per_epoch=20, validation_data = validation_generator, verbose = 1, validation_steps=3)\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yWbmET0xCZLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BLywYzTdCjIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Prediction**"
      ],
      "metadata": {
        "id": "VOBo_eIzCk8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE: If you are using Safari and this cell throws an error,\n",
        "## please skip this block and run the next one instead.\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = load_img(path, target_size=(150, 150))\n",
        "  x = img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "metadata": {
        "id": "z4f2w5GRD6qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CODE BLOCK FOR OLDER VERSIONS OF SAFARI\n",
        "\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "# images = os.listdir(\"/tmp/images\")\n",
        "\n",
        "# print(images)\n",
        "\n",
        "# for i in images:\n",
        "#     print()\n",
        "#     # predicting images\n",
        "#     path = '/tmp/images/' + i\n",
        "#     img = load_img(path, target_size=(150, 150))\n",
        "#     x = img_to_array(img)\n",
        "#     x = np.expand_dims(x, axis=0)\n",
        "\n",
        "#     images = np.vstack([x])\n",
        "#     classes = model.predict(images, batch_size=10)\n",
        "#     print(path)\n",
        "#     print(classes)"
      ],
      "metadata": {
        "id": "n27tIpdkD9ME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}